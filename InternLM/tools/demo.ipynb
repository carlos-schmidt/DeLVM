{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc90fb20-685c-4649-bae1-614ec2d45926",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 1: convert imternlm model to hf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecea625-1a52-4615-94ce-2f068b06546b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python convert2hf.py --src_folder /path/to/intermlm_model/ --tgt_folder /path/to/save/hf_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674bb47-59ac-4418-baa5-d9a99c1eeb8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step2: Prompted inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56d9fe-e74d-46e9-adf0-b50957b6ae18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare parameters\n",
    "prompt_path='./data/seg_prompt/'  # path to prompt\n",
    "input_img='./data/examples/seg_1.png'  # path to input image\n",
    "\n",
    "lvm_path='../../models/llama_300m_hf'  # path to converted hf model\n",
    "vqgan_path='../../models/vqgan-f16-8192-laion'  # path to vqgan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bd8204-146b-4383-baf7-b9e4434bc8d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, GenerationConfig\n",
    "\n",
    "from model_hf.muse import VQGANModel\n",
    "from utils import convert_decode_to_pil, encode_transform, patchify, unpatchify\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4060b444-061d-41b0-b919-fe0105c55f36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare models and config\n",
    "vq_model = VQGANModel.from_pretrained(vqgan_path).to(DEVICE).eval()\n",
    "model = AutoModel.from_pretrained(lvm_path, trust_remote_code=True).to(DEVICE).eval()\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        num_beams=1,\n",
    "        early_stopping=True,\n",
    "        max_new_tokens=256,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbaecf-31ae-4711-a714-d75b064798f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare prompt\n",
    "img_names = os.listdir(prompt_path)\n",
    "img_names = sorted(img_names)\n",
    "\n",
    "seq_prompt, names = [], []\n",
    "for i, img_name in enumerate(img_names):\n",
    "    print('prompt: ', img_name)\n",
    "    img_path = os.path.join(prompt_path, img_name)\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "    image = encode_transform(image)\n",
    "    image = image[0:3,:,:].unsqueeze(0)\n",
    "    seq_prompt.append(image)\n",
    "\n",
    "seq_ids = []\n",
    "for images in seq_prompt:\n",
    "    images = images.to(DEVICE)\n",
    "\n",
    "    # tokenize\n",
    "    quantized_states, indices = vq_model.encode(images)\n",
    "    prompt_ids = indices.reshape(1, -1)\n",
    "    seq_ids.append(prompt_ids)\n",
    "\n",
    "seq_ids = torch.cat(seq_ids, dim=1)\n",
    "\n",
    "print(type(seq_ids), seq_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51d90c-7655-4598-97ad-3deefd2c696b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare input\n",
    "if type(input_img) is str:\n",
    "    input_img = Image.open(input_img)\n",
    "img = encode_transform(input_img)[0:3,:,:].unsqueeze(0).to(DEVICE)\n",
    "quantized_states, indices = vq_model.encode(img)\n",
    "input_ids = indices.reshape(1, -1)\n",
    "input_ids = torch.cat([seq_ids, input_ids], dim=1)\n",
    "\n",
    "print(type(input_ids), input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0f68b-a9ee-423b-bedc-ef61d789cc2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=input_ids,\n",
    "                             generation_config=generation_config,\n",
    "                             max_new_tokens=256,\n",
    "                             return_dict_in_generate=True,\n",
    "                             output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf738f-7381-4cd8-9214-a4438db038ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "generated_tokens = vq_model.quantize.get_codebook_entry_for_lvm(outputs.sequences[:, -256:])\n",
    "generated_tokens = generated_tokens.view(1, generated_tokens.shape[1] // 16, 16, -1).permute(0, 3, 1, 2)\n",
    "generated_img = vq_model.decode(generated_tokens)\n",
    "generated_img_rec = convert_decode_to_pil(generated_img)[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(input_img)\n",
    "axes[1].imshow(generated_img_rec)\n",
    "for ax in axes:\n",
    "    ax.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
